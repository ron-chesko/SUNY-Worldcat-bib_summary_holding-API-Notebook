{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using the WorldCat API to integrate holdings information and ILS data\n",
        "## WorldCat API bit\n",
        "How to query the bib-summary-holdings endpoint with Oauth2 tokens. (Python version using a Google Colab notebook)\n",
        "\n",
        "## Loading in our Python libraries\n",
        "If you've never used Python notebooks, this might be something new. You have the ability to break up your code into blocks. This makes it nice to not only organize code but it saves you from re-executing code blocks that only need to be run once. In the case of Python, we don't really need to load libraries and setup global things over and over in a notebook because they are persistent.\n",
        "\n",
        "So, it's common to set up a block with your libraries and other things that only need to be run once. We only need to \"press play\" on this once per session.\n",
        "\n",
        "I'm using the requests library do handle all of the HTTP GET requests.\n",
        "Pandas is a commonly used library for handling table-like data.\n",
        "I'm using the google.colab library to access my Google Drive to get and save files.\n",
        "The csv library does what's \"on the tin\", allows you to work with CSV files.\n",
        "Datetime allows you to easily work with date and time fields.\n"
      ],
      "metadata": {
        "id": "ZIPZrOsLX-Ay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvW4Ca1R6H7_",
        "outputId": "426d65f6-bd7c-43e3-8e21-33778b0088fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "# Code for processing Worldcat API\n",
        "\n",
        "# Python \"requests\", simple HTTP library. https://pypi.org/project/requests/\n",
        "import requests\n",
        "\n",
        "# Pandas data analysis library\n",
        "import pandas as pd\n",
        "\n",
        "# Allow access to files and folders in google drive\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "# CSV library for importing and expoerting to CSV\n",
        "import csv\n",
        "\n",
        "# Datetime library for handling dates and times\n",
        "import datetime\n",
        "\n",
        "# Mount the google drive for access in the code\n",
        "# Note: Google will ask for permission to access the google drive\n",
        "drive.mount('/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens\n",
        "As I mention in the comments below, the bib-summary-holdings endpoint works on a token system. This allows for increased security but is a bit of a speed bump when you're trying to use the endpoint to retrieve a lot of records.\n",
        "\n",
        "Have a look at the inline comments but the TL;DR is that this function gets a security token and then renews the token as needed.\n",
        "\n",
        "Since this is a function we only need to \"press play\" on this once per session as well. Note: if we do make any changes, we will have to run it again for the changes to take effect."
      ],
      "metadata": {
        "id": "xYGqYiptiIe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getToken - Function for fetching and refreshing a token from Worldcat. Tokens are required for v2 API endpoints.\n",
        "#This is called to get a new token at the start and refresh the token as needed.\n",
        "#Note: The WorldCat API seems to be built for single transactions to the token timeout is fairly short.\n",
        "\n",
        "def getToken(refreshTkn=None):\n",
        "\n",
        "    # Set up the base URL and credentials. These creds are Ron's\n",
        "    tokenUrl = 'https://oauth.oclc.org/token?'\n",
        "    client_id = '[Worldcat Client ID goes here]'\n",
        "    client_secret = '[Worldcat Client Secret goes here]'\n",
        "\n",
        "    # First check if this is already a refresh token. If not, we're generating a new one.\n",
        "    if refreshTkn is None:\n",
        "\n",
        "        # Append the correct parameters to the URL, post the request, grab and assign the token info we need.\n",
        "        tokenUrl = tokenUrl + 'scope=wcapi+refresh_token&grant_type=client_credentials'\n",
        "        tokenResp = requests.post(tokenUrl, auth=(client_id, client_secret))\n",
        "        tokenJson = tokenResp.json()\n",
        "        apiToken = tokenJson[\"access_token\"]\n",
        "        refreshToken = tokenJson[\"refresh_token\"]\n",
        "        tokenExp = tokenJson[\"expires_at\"]\n",
        "        return apiToken, refreshToken, tokenExp;\n",
        "\n",
        "    # Ok, this IS a refresh token.\n",
        "    else:\n",
        "\n",
        "        # Different parameters for a refresh token. Again, assign parameters, post request and assign the info returned.\n",
        "        tokenUrl = tokenUrl + 'refresh_token=' + refreshTkn + '&grant_type=refresh_token'\n",
        "        tokenResp = requests.post(tokenUrl, auth=(client_id, client_secret))\n",
        "        tokenJson = tokenResp.json()\n",
        "        apiToken = tokenJson[\"access_token\"]\n",
        "        refreshToken = tokenJson[\"refresh_token\"]\n",
        "        tokenExp = tokenJson[\"expires_at\"]\n",
        "        return apiToken, refreshToken, tokenExp;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CYbMD6D4DVO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Data\n",
        "Again, lots more technical details in the code comments but this is the main bit of code that:\n",
        "- Fetches the OCLC numbers from a CSV file\n",
        "- Generates the initial token\n",
        "- Loops through the OCLC numbers we want to query the API\n",
        "- Look through the data returned by the API and get the specific stuff we want\n",
        "- Along the way, watch the time and refresh the token as needed\n",
        "- Finally, write all of our data out to another CSV\n",
        "\n",
        "Note: This is the block of code that we may run over and over again as we make changes to the data or code.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcWAP0VMi8g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main bit of the code\n",
        "\n",
        "# Open up the input CSV file in the google drive. I tend to save everything in the Colab Notebooks folder just\n",
        "# for ease of access but files can be saved anywhere. The \"root\" of the google drive is /drive/My Drive/\n",
        "# read_csv is a pandas function that parses a CSV and allows you to assign it to a data frame\n",
        "data = pd.read_csv('/drive/My Drive/Colab Notebooks/oclc_numbers.csv')\n",
        "\n",
        "# drop_duplicates removes duplicate rows from the dataframe. I assign this to a new data frame\n",
        "# so I can access the original data if needed.\n",
        "data2 = data.drop_duplicates()\n",
        "\n",
        "# Create a list variable for output\n",
        "output = list()\n",
        "\n",
        "# Get our initial token and grab the info we need to refresh when needed.\n",
        "aToken, rToken, tExp = getToken()\n",
        "\n",
        "# The main loop. Iterate through the rows that we got from the CSV\n",
        "for index, row in data2.iterrows():\n",
        "\n",
        "    # This block of code compares the expiration time on the token to the current time\n",
        "    # If the expiration is less than a minute away, refresh the token\n",
        "\n",
        "    # Find the current data and time.\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    # Add a minute to it. Not sure why I called the variable twoMin\n",
        "    twoMin = now + datetime.timedelta(minutes=1)\n",
        "\n",
        "    # Convert the token expiration date to the same format as the current date (\"now\")\n",
        "    exp = datetime.datetime.strptime(tExp, '%Y-%m-%d %H:%M:%SZ')\n",
        "\n",
        "    # Check if token expiration time is less than 1 minute away. If it is, call getToken()\n",
        "    # to renew the token\n",
        "    if (twoMin>exp):\n",
        "        aToken, rToken, tExp = getToken(rToken)\n",
        "\n",
        "    # Grab the next oclc numeber out of the correct column (Make sure the column name in the CSV matches what's in the row identifier)\n",
        "    oclcnum = str(row['OCLC Number (035a)'])\n",
        "\n",
        "    # Create our URL with the oclc number inserted\n",
        "    url = 'https://americas.discovery.api.oclc.org/worldcat/search/v2/bibs-summary-holdings?&oclcNumber=' + oclcnum + '&holdingsAllEditions=false&preferredLanguage=eng'\n",
        "\n",
        "    # Run a http get request with the ULR we just created\n",
        "    response = requests.get(url, headers={'Authorization': 'Bearer ' + aToken})\n",
        "\n",
        "    # Commented out print statement for troubleshooting\n",
        "    # print(index)\n",
        "\n",
        "    # Grab the json data returned from the get request\n",
        "    jsonData = response.json()\n",
        "\n",
        "    # Drill down through the JSON structure to find the count we're looking for\n",
        "    count = jsonData[\"briefRecords\"][0][\"institutionHolding\"][\"totalHoldingCount\"]\n",
        "\n",
        "    # Commented out print statement for troubleshooting\n",
        "    #print(jsonData)\n",
        "\n",
        "    # Create a comma separated row of our data. In this case the oclc number and count we just got\n",
        "    rowData = [oclcnum, count]\n",
        "\n",
        "    # Append the data to our output list\n",
        "    output.append(rowData)\n",
        "\n",
        "# Create a new data frame with out output list. Add column headings\n",
        "df = pd.DataFrame(output, columns=[\"oclcnum\",\"totalHoldingCount\"])\n",
        "\n",
        "# Output the dataframe to a CSV on our google drive. Output parameter of QUOTE_ALL. This means that the CSV\n",
        "# will put double quotes around all of the data so it's all treated as text when importing. Sometimes long\n",
        "# OCLC numbers (as well as other IDs, like MMSIDs) are mistaken for large numbers and converted to exponetial\n",
        "# notation which tends to mess up IDs in Excel. Easier to treat it all as text and chenge to numbers as needed\n",
        "# once it's in excel.\n",
        "df.to_csv('/drive/My Drive/Colab Notebooks/oclc_holdingcounts_20230207.csv', quoting=csv.QUOTE_ALL)\n",
        "\n"
      ],
      "metadata": {
        "id": "zVy2Ux6iAbpX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
